{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Notebook Imports\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "### Set Notebook Parameters\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"images/model_selection_step.png\" alt=\"Model Selection\" style=\"width: 1000px;\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection for the Pairy Application\n",
    "\n",
    "## Building a Recommendation Engine using BERT\n",
    "\n",
    "### Introduction\n",
    "We will explore how to leverage BERT (Bidirectional Encoder Representations from Transformers) to build an advanced recommendation engine for the \"Pairy - influencer matchmaking\" app. BERT is a pre-trained transformer-based model that excels in understanding the context of words in a sentence, making it an ideal choice for tasks involving natural language understanding.\n",
    "\n",
    "### Steps to Build the Recommendation Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## `1.Data Preparation:`\n",
    "- Gather user interaction data, influencer profiles, and any other relevant textual information.\n",
    "- Preprocess and tokenize the text data, ensuring it's in a format that BERT can process.\n",
    "\n",
    "<details>\n",
    "<summary><b>Expand: Data Preparation</b></summary>\n",
    "\n",
    "### Gather User Interaction Data and Influencer Profiles\n",
    "- Collect user interaction data, which might include user actions such as likes, shares, comments, or preferences.\n",
    "- Gather influencer profiles containing information about the influencer's topics, content, engagement, and other attributes.\n",
    "\n",
    "### Preprocess and Tokenize Text Data\n",
    "- Preprocess the gathered text data to remove noise, special characters, and irrelevant information.\n",
    "- Tokenize the cleaned text data into subword tokens using BERT's tokenizer.\n",
    "- BERT's tokenizer breaks text into tokens, and each token is assigned a unique ID that the model understands.\n",
    "\n",
    "### Tokenization Steps\n",
    "- **Token Subword Splitting**: Split words into smaller subwords (e.g., \"running\" into \"run\" and \"##ning\").\n",
    "- **Vocabulary Mapping**: Map subwords to token IDs based on BERT's vocabulary.\n",
    "- **Special Tokens**: Add special tokens like `[CLS]` (start), `[SEP]` (separator), and `[PAD]` (padding).\n",
    "- **Attention Masks**: Create attention masks to distinguish between content and padding tokens.\n",
    "\n",
    "### Example Python Code for Tokenization\n",
    "\n",
    "```python\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# Initialize BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Example text\n",
    "text = \"User A likes influencers who talk about topic X.\"\n",
    "\n",
    "# Tokenize and encode text\n",
    "encoded_input = tokenizer.encode_plus(text, add_special_tokens=True, max_length=512, pad_to_max_length=True)\n",
    "input_ids = encoded_input['input_ids']\n",
    "attention_mask = encoded_input['attention_mask']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `2.Fine-Tuning BERT`\n",
    "- Fine-tune a pre-trained BERT model on our specific recommendation task.\n",
    "- Utilize a dataset containing pairs of user interactions and influencer profiles, along with labels indicating successful matches.\n",
    "- The goal is to help BERT understand the relationship between user interactions and influencer profiles that lead to matches.\n",
    "\n",
    "\n",
    "<details>\n",
    "<summary><b>Expand: Fine-Tuning BERT</b></summary>\n",
    "\n",
    "### Fine-Tuning Process\n",
    "- Fine-tuning involves training a pre-trained BERT model on our specific recommendation task.\n",
    "- The model learns to understand the nuances of user interactions and influencer profiles to make accurate match predictions.\n",
    "\n",
    "### Dataset Preparation\n",
    "- Create a dataset containing pairs of user interactions and influencer profiles.\n",
    "- Include labels indicating whether a pair resulted in a successful match or not.\n",
    "- This labeled dataset forms the foundation for training the recommendation model.\n",
    "\n",
    "### Training Objective\n",
    "- Define a suitable training objective, often a binary classification problem.\n",
    "- The model learns to predict whether a given pair of user interaction and influencer profile is a successful match or not.\n",
    "\n",
    "### Model Adaptation\n",
    "- Fine-tuning adjusts the pre-trained BERT model's weights using the labeled dataset.\n",
    "- During fine-tuning, the model adapts to the specific patterns and relationships present in our recommendation task.\n",
    "\n",
    "### Python Code for Fine-Tuning BERT\n",
    "```python\n",
    "from transformers import BertForSequenceClassification, BertTokenizer, AdamW\n",
    "import torch\n",
    "\n",
    "# Load pre-trained BERT model\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Define optimizer and loss function\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Train the model using the labeled dataset\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in dataloader:\n",
    "        inputs, labels = batch\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(**inputs)\n",
    "        loss = loss_fn(outputs.logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `3. Embedding Generation`\n",
    "- Generate embeddings (dense vector representations) for both user interactions and influencer profiles using the fine-tuned BERT model.\n",
    "- These embeddings capture the semantic meaning and context of the text data, enabling accurate similarity calculations.\n",
    "\n",
    "\n",
    "<details>\n",
    "<summary><b>Expand: Embedding Generation</b></summary>\n",
    "\n",
    "### Generate Embeddings\n",
    "- Utilize the fine-tuned BERT model to generate embeddings for user interactions and influencer profiles.\n",
    "- Embeddings are dense vector representations that capture the semantic meaning and context of the text data.\n",
    "\n",
    "### Semantic Meaning and Context\n",
    "- BERT's embeddings encode the intricate relationships between words and their surrounding context.\n",
    "- By capturing semantic meaning and context, the embeddings enhance the quality of similarity calculations.\n",
    "\n",
    "### Calculating Embeddings\n",
    "- Pass the tokenized input sequences through the fine-tuned BERT model.\n",
    "- Extract the hidden states or pooled output from the model to obtain embeddings.\n",
    "\n",
    "### Similarity Calculations\n",
    "- Once we have our embeddings for user interactions and influencer profiles, calculate similarity scores.\n",
    "- Common similarity metrics include cosine similarity, Euclidean distance, or dot product.\n",
    "\n",
    "### Python Code for Embedding Generation\n",
    "```python\n",
    "# Assuming 'model' is our fine-tuned BERT model\n",
    "input_sequences = [\"User A likes influencers who talk about topic X.\", ...]  # List of input sequences\n",
    "input_ids = [tokenizer.encode(seq, add_special_tokens=True, max_length=512, pad_to_max_length=True) for seq in input_sequences]\n",
    "\n",
    "# Generate embeddings for input sequences\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    input_tensors = torch.tensor(input_ids)\n",
    "    outputs = model(input_tensors)\n",
    "    embeddings = outputs.last_hidden_state  # Extract embeddings from BERT's last hidden state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `4. Similarity Calculation`\n",
    "- Calculate similarity scores between user embeddings and influencer embeddings using methods like cosine similarity.\n",
    "- Higher similarity scores indicate stronger potential matches.\n",
    "\n",
    "<details>\n",
    "<summary><b>Expand: Similarity Calculation</b></summary>\n",
    "\n",
    "### Calculate Similarity Scores\n",
    "- After obtaining user and influencer embeddings, calculate similarity scores.\n",
    "- Common similarity metrics like cosine similarity, Euclidean distance, or dot product can be used.\n",
    "\n",
    "### Cosine Similarity\n",
    "- Cosine similarity measures the cosine of the angle between two vectors.\n",
    "- It quantifies the similarity of direction between vectors, irrespective of their magnitudes.\n",
    "\n",
    "### Interpreting Similarity Scores\n",
    "- Higher similarity scores indicate stronger potential matches.\n",
    "- Similarity scores close to 1 suggest a high degree of similarity, while scores close to 0 suggest dissimilarity.\n",
    "\n",
    "### Python Code for Similarity Calculation (Cosine Similarity)\n",
    "```python\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Calculate cosine similarity between user and influencer embeddings\n",
    "user_embedding = embeddings[user_index]  # Replace with actual user embedding\n",
    "influencer_embedding = embeddings[influencer_index]  # Replace with actual influencer embedding\n",
    "\n",
    "similarity_score = cosine_similarity([user_embedding], [influencer_embedding])[0][0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `5. Ranking and Recommendation`\n",
    "- Rank influencers based on their similarity scores to the user.\n",
    "- Select the top-N influencers with the highest similarity scores as recommendations.\n",
    "\n",
    "\n",
    "<details>\n",
    "<summary><b>Expand: Ranking and Recommendation</b></summary>\n",
    "\n",
    "### Rank Influencers\n",
    "- Based on the calculated similarity scores, rank influencers relative to the user.\n",
    "- Influencers with higher similarity scores are ranked higher as potential matches.\n",
    "\n",
    "### Top-N Recommendations\n",
    "- Select the top-N influencers with the highest similarity scores as recommendations.\n",
    "- The value of N can be determined based on user preferences and the desired number of recommendations.\n",
    "\n",
    "### Presentation to Users\n",
    "- Present the ranked influencers and their details to the user in an organized manner.\n",
    "- Display information such as influencer names, topics, and a brief description.\n",
    "\n",
    "### Personalization\n",
    "- Consider incorporating personalization by fine-tuning the model with user-specific data.\n",
    "- Personalized models can enhance the quality and relevance of recommendations.\n",
    "\n",
    "### Python Code for Ranking and Recommendation\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# Calculate similarity scores for all influencers\n",
    "user_embedding = embeddings[user_index]  # Replace with actual user embedding\n",
    "influencer_embeddings = embeddings[influencer_indices]  # Replace with actual influencer embeddings\n",
    "\n",
    "similarity_scores = cosine_similarity([user_embedding], influencer_embeddings)[0]\n",
    "\n",
    "# Rank influencers based on similarity scores\n",
    "sorted_indices = np.argsort(similarity_scores)[::-1]  # Descending order\n",
    "top_n_indices = sorted_indices[:n]  # Select top-N indices for recommendations\n",
    "\n",
    "# Get influencer details for top-N recommendations\n",
    "top_n_influencers = [influencers[index] for index in top_n_indices]  # Replace with actual influencer data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `6. Personalization`\n",
    "- Leverage BERT's contextual understanding to incorporate user-specific information into embeddings.\n",
    "- Include user preferences, past interactions, and questionnaire responses as input to BERT for personalized recommendations.\n",
    "\n",
    "<details>\n",
    "<summary><b>Expand: Personalization</b></summary>\n",
    "\n",
    "### Incorporate User-Specific Information\n",
    "- Utilize BERT's contextual understanding to personalize recommendations.\n",
    "- Include user-specific data such as preferences, past interactions, and questionnaire responses.\n",
    "\n",
    "### Enhanced User Embeddings\n",
    "- Create enhanced user embeddings by combining standard user embeddings with personalized features.\n",
    "- Concatenate or sum the embeddings to create a comprehensive representation.\n",
    "\n",
    "### Fine-Tuning for Personalization\n",
    "- Fine-tune the BERT model with user-specific data for enhanced recommendation quality.\n",
    "- The personalized model learns to capture individual preferences and interactions.\n",
    "\n",
    "### Questionnaire Responses\n",
    "- Utilize user responses to questionnaires or surveys as additional input.\n",
    "- Process and tokenize questionnaire responses similar to other text data.\n",
    "\n",
    "### Python Code for Personalization\n",
    "```python\n",
    "# Assuming 'model' is our fine-tuned BERT model\n",
    "\n",
    "# Combine standard user embedding with personalized features\n",
    "personalized_user_embedding = torch.cat([user_embedding, personalized_features], dim=1)\n",
    "\n",
    "# Generate personalized recommendations using the personalized user embedding\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    personalized_outputs = model(personalized_user_embedding)\n",
    "    personalized_embeddings = personalized_outputs.last_hidden_state\n",
    "\n",
    "# Calculate similarity scores and rank personalized recommendations\n",
    "# This follows similar steps as in the \"Ranking and Recommendation\" section\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `7. Evaluation and Iteration`\n",
    "- Evaluate the recommendation engine's performance using metrics such as precision, recall, and F1-score.\n",
    "- Fine-tune model parameters, experiment with configurations, and gather user feedback for continuous improvement.\n",
    "\n",
    "\n",
    "<details>\n",
    "<summary><b>Expand: Evaluation and Iteration</b></summary>\n",
    "\n",
    "### Performance Evaluation\n",
    "- Evaluate the recommendation engine's performance using various metrics.\n",
    "- Common evaluation metrics include precision, recall, F1-score, NDCG, hit-ratio, and top-N accuracy.\n",
    "\n",
    "### User Feedback\n",
    "- Gather user feedback to understand the quality and relevance of recommendations.\n",
    "- User input helps identify areas for improvement and potential shortcomings.\n",
    "\n",
    "### Fine-Tuning and Parameter Optimization\n",
    "- Continuously fine-tune model parameters to enhance recommendation accuracy.\n",
    "- Experiment with hyperparameters, model architectures, and data preprocessing techniques.\n",
    "\n",
    "### A/B Testing\n",
    "- Conduct A/B testing to compare different recommendation strategies.\n",
    "- Compare the performance of different model versions or algorithms using real-world user interactions.\n",
    "\n",
    "### Iterative Development\n",
    "- Use a continuous development cycle to iterate on the recommendation engine.\n",
    "- Incorporate insights from evaluation, feedback, and experiments to make improvements.\n",
    "\n",
    "### Python Code for Evaluation Metrics (including NDCG and Hit-ratio)\n",
    "```python\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import ndcg_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Calculate evaluation metrics for recommendations\n",
    "true_labels = [1, 0, 1, ...]  # Actual labels indicating match (1) or not (0)\n",
    "predicted_labels = [1, 1, 0, ...]  # Predicted labels by the recommendation engine\n",
    "\n",
    "precision = precision_score(true_labels, predicted_labels)\n",
    "recall = recall_score(true_labels, predicted_labels)\n",
    "f1 = f1_score(true_labels, predicted_labels)\n",
    "\n",
    "# Calculate NDCG (Normalized Discounted Cumulative Gain)\n",
    "ndcg = ndcg_score([true_labels], [predicted_labels])\n",
    "\n",
    "# Calculate Hit-ratio\n",
    "hit_ratio = accuracy_score(true_labels, predicted_labels)\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-score:\", f1)\n",
    "print(\"NDCG:\", ndcg)\n",
    "print(\"Hit-ratio:\", hit_ratio)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Considerations and Conclusion`\n",
    "- BERT can be resource-intensive, so consider using smaller transformer models like DistilBERT or ALBERT for efficiency.\n",
    "- While BERT enhances content-based recommendations, collaborative filtering techniques may complement explicit user-item interactions.\n",
    "- Building a recommendation engine with BERT promises to elevate the \"Pairy\" app's user experience and engagement, catering to users seeking meaningful influencer partnerships.\n",
    "\n",
    "\n",
    "<details>\n",
    "<summary><b>Expand: Considerations and Conclusion</b></summary>\n",
    "\n",
    "### Model Selection\n",
    "- While BERT is powerful, it can be resource-intensive due to its large size.\n",
    "- Consider using smaller transformer models like DistilBERT or ALBERT for efficient recommendations.\n",
    "\n",
    "### Complementary Techniques\n",
    "- BERT enhances content-based recommendations by understanding text data.\n",
    "- Consider combining BERT-based recommendations with collaborative filtering techniques for a holistic approach.\n",
    "\n",
    "### Enhanced User Experience\n",
    "- Building a recommendation engine with BERT can lead to a more engaging user experience.\n",
    "- Users seeking meaningful influencer partnerships can benefit from personalized and relevant recommendations.\n",
    "\n",
    "### Continuous Improvement\n",
    "- Remember that recommendation systems are iterative projects.\n",
    "- Continuously gather user feedback, analyze metrics, and refine the engine for optimal results.\n",
    "\n",
    "### Conclusion\n",
    "- Incorporating BERT into the recommendation engine of the \"Pairy\" app offers a unique opportunity.\n",
    "- By leveraging BERT's contextual understanding, you can provide users with tailored influencer recommendations, ultimately enhancing the app's value proposition.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vs. Using BART Model for Building a Recommendation Engine\n",
    "\n",
    "## Introduction\n",
    "We we also explore the potential of leveraging the BART (Bidirectional and Auto-Regressive Transformers) model to enhance the recommendation engine for the \"Pairy - influencer matchmaking\" app. BART is a transformer-based model designed for sequence-to-sequence tasks, making it suitable for tasks involving text generation and summarization.\n",
    "\n",
    "## Steps to Utilize BART for Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `1.Data Preparation:`\n",
    "- Gather user interaction data and influencer profiles.\n",
    "- Format the data into pairs of input and target sequences, similar to sequence-to-sequence tasks.\n",
    "- Input sequence: User preferences or interactions.\n",
    "- Target sequence: Influencer attributes or summary.\n",
    "\n",
    "\n",
    "<details>\n",
    "<summary><b>Expand: Data Preparation</b></summary>\n",
    "\n",
    "### Gather User Interaction Data and Influencer Profiles\n",
    "- Collect user interaction data, which could include likes, comments, shares, or preferences.\n",
    "- Gather detailed influencer profiles containing attributes like topics, engagement, content style, and more.\n",
    "\n",
    "### Format Data into Sequence Pairs\n",
    "- Prepare the data in pairs of input and target sequences.\n",
    "- Input Sequence: Represent user preferences, interactions, or interests.\n",
    "- Target Sequence: Include influencer attributes or summaries relevant to the user's interests.\n",
    "\n",
    "### Sequence-to-Sequence Task\n",
    "- Formulate the task as a sequence-to-sequence problem.\n",
    "- The model will learn to generate influencer recommendations based on user inputs.\n",
    "\n",
    "### Example Data Pair:\n",
    "- **Input Sequence**: \"User A is interested in fashion and beauty content.\"\n",
    "- **Target Sequence**: \"Influencer X specializes in fashion and beauty, with a strong online presence.\"\n",
    "\n",
    "### Python Code for Data Formatting\n",
    "```python\n",
    "# Example user interaction and influencer profile data\n",
    "user_interactions = [\"User A likes fashion content.\", \"User B follows travel influencers.\", ...]\n",
    "influencer_profiles = [\"Influencer X: Fashion and beauty expert.\", \"Influencer Y: Adventure travel enthusiast.\", ...]\n",
    "\n",
    "# Format data into sequence pairs\n",
    "data_pairs = [(interaction, profile) for interaction, profile in zip(user_interactions, influencer_profiles)]\n",
    "input_sequences = [pair[0] for pair in data_pairs]\n",
    "target_sequences = [pair[1] for pair in data_pairs]\n",
    "\n",
    "# Print example data pair\n",
    "print(\"Example Input Sequence:\", input_sequences[0])\n",
    "print(\"Example Target Sequence:\", target_sequences[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `2.Tokenization:`\n",
    "- Tokenize input and target sequences using BART's tokenizer.\n",
    "- Add special tokens like [BOS] (beginning of sequence) and [EOS] (end of sequence).\n",
    "- Generate attention masks to identify content and padding tokens.\n",
    "\n",
    "<details>\n",
    "<summary><b>Expand: Tokenization</b></summary>\n",
    "\n",
    "### Tokenize Input and Target Sequences\n",
    "- Utilize BART's tokenizer to convert input and target sequences into tokens.\n",
    "- Tokenization converts text into subword tokens recognizable by the model.\n",
    "\n",
    "### Special Tokens\n",
    "- Add special tokens to represent the beginning and end of sequences.\n",
    "- Use `[BOS]` (beginning of sequence) and `[EOS]` (end of sequence) tokens.\n",
    "\n",
    "### Attention Masks\n",
    "- Generate attention masks to differentiate content tokens from padding tokens.\n",
    "- Attention masks help the model focus on meaningful content and ignore padding.\n",
    "\n",
    "### Python Code for Tokenization\n",
    "```python\n",
    "from transformers import BartTokenizer\n",
    "\n",
    "# Initialize BART tokenizer\n",
    "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large\")\n",
    "\n",
    "# Example input and target sequences\n",
    "input_sequence = \"User A is interested in fashion and beauty content.\"\n",
    "target_sequence = \"Influencer X specializes in fashion and beauty, with a strong online presence.\"\n",
    "\n",
    "# Tokenize input and target sequences\n",
    "input_tokens = tokenizer.encode(\"[BOS] \" + input_sequence + \" [EOS]\", add_special_tokens=False)\n",
    "target_tokens = tokenizer.encode(\"[BOS] \" + target_sequence + \" [EOS]\", add_special_tokens=False)\n",
    "\n",
    "# Generate attention masks\n",
    "input_attention_mask = [1] * len(input_tokens)\n",
    "target_attention_mask = [1] * len(target_tokens)\n",
    "\n",
    "print(\"Input Tokens:\", input_tokens)\n",
    "print(\"Target Tokens:\", target_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `3.Model Fine-Tuning:`\n",
    "- Fine-tune a pre-trained BART model on our recommendation task.\n",
    "- Use the pairs of input and target sequences in the training data.\n",
    "- The model learns to generate meaningful influencer attributes or summaries.\n",
    "\n",
    "<details>\n",
    "<summary><b>Expand: Model Fine-Tuning</b></summary>\n",
    "\n",
    "### Fine-Tuning Process\n",
    "- Fine-tune a pre-trained BART model for our recommendation task.\n",
    "- The model learns to generate influencer attributes or summaries based on user inputs.\n",
    "\n",
    "### Training Data\n",
    "- Use the pairs of input and target sequences in the training data.\n",
    "- Input sequences represent user preferences or interactions.\n",
    "- Target sequences contain influencer attributes or summaries.\n",
    "\n",
    "### Training Objective\n",
    "- Define a suitable training objective for sequence-to-sequence generation.\n",
    "- BART aims to minimize the difference between generated and target sequences.\n",
    "\n",
    "### Python Code for Model Fine-Tuning\n",
    "```python\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer, AdamW\n",
    "import torch\n",
    "\n",
    "# Initialize pre-trained BART model and tokenizer\n",
    "model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large\")\n",
    "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large\")\n",
    "\n",
    "# Example training data (input and target sequences)\n",
    "input_sequence = \"User A is interested in fashion and beauty content.\"\n",
    "target_sequence = \"Influencer X specializes in fashion and beauty, with a strong online presence.\"\n",
    "\n",
    "# Tokenize input and target sequences\n",
    "input_tokens = tokenizer.encode(\"[BOS] \" + input_sequence + \" [EOS]\", add_special_tokens=False)\n",
    "target_tokens = tokenizer.encode(\"[BOS] \" + target_sequence + \" [EOS]\", add_special_tokens=False)\n",
    "\n",
    "# Convert tokens to tensors\n",
    "input_tensor = torch.tensor([input_tokens])\n",
    "target_tensor = torch.tensor([target_tokens])\n",
    "\n",
    "# Fine-tune the model\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    logits = model(input_tensor).logits\n",
    "    loss = loss_fn(logits.view(-1, logits.shape[-1]), target_tensor.view(-1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `4.Recommendation Generation:`\n",
    "- Given a user's preferences or interactions, generate influencer recommendations.\n",
    "- Use the fine-tuned BART model to generate target sequences based on the input.\n",
    "- The generated sequences are potential recommendations.\n",
    "\n",
    "<details>\n",
    "<summary><b>Expand: Recommendation Generation</b></summary>\n",
    "\n",
    "### Generate Influencer Recommendations\n",
    "- Utilize the fine-tuned BART model to generate influencer recommendations.\n",
    "- Given a user's preferences or interactions, the model generates target sequences.\n",
    "\n",
    "### Model Inference\n",
    "- Pass the input sequence through the fine-tuned BART model.\n",
    "- The model generates a sequence representing an influencer recommendation.\n",
    "\n",
    "### Potential Recommendations\n",
    "- The generated sequences are potential influencer recommendations.\n",
    "- The model learns to generate meaningful summaries or attributes.\n",
    "\n",
    "### Python Code for Recommendation Generation\n",
    "```python\n",
    "# Example input sequence from user interactions\n",
    "user_input = \"User A is interested in fashion and beauty content.\"\n",
    "\n",
    "# Tokenize user input\n",
    "input_tokens = tokenizer.encode(\"[BOS] \" + user_input + \" [EOS]\", add_special_tokens=False)\n",
    "\n",
    "# Convert tokens to tensor\n",
    "input_tensor = torch.tensor([input_tokens])\n",
    "\n",
    "# Generate influencer recommendation using the fine-tuned model\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    generated_tokens = model.generate(input_tensor)\n",
    "    generated_sequence = tokenizer.decode(generated_tokens[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Generated Recommendation:\", generated_sequence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `5.Scoring and Ranking:`\n",
    "- Calculate relevance scores for generated recommendations.\n",
    "- You can use metrics like cosine similarity, Jaccard similarity, or custom relevance scores.\n",
    "\n",
    "\n",
    "<details>\n",
    "<summary><b>Expand: Scoring and Ranking</b></summary>\n",
    "\n",
    "### Calculate Relevance Scores\n",
    "- After generating influencer recommendations, calculate relevance scores.\n",
    "- Relevance scores quantify the suitability of each recommendation.\n",
    "\n",
    "### Similarity Metrics\n",
    "- Use similarity metrics like cosine similarity or Jaccard similarity.\n",
    "- These metrics compare the generated recommendation with user preferences.\n",
    "\n",
    "### Custom Relevance Scores\n",
    "- Develop custom relevance scores based on domain-specific criteria.\n",
    "- Consider attributes like topic match, engagement, or user interactions.\n",
    "\n",
    "### Python Code for Scoring and Ranking (Cosine Similarity)\n",
    "```python\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Example generated recommendation\n",
    "generated_recommendation = \"Influencer X specializes in fashion and beauty, with a strong online presence.\"\n",
    "\n",
    "# Tokenize and convert to tensor\n",
    "generated_tokens = tokenizer.encode(\"[BOS] \" + generated_recommendation + \" [EOS]\", add_special_tokens=False)\n",
    "generated_tensor = torch.tensor([generated_tokens])\n",
    "\n",
    "# Calculate cosine similarity with user input\n",
    "user_input_embedding = model.get_input_embeddings()(input_tensor).mean(dim=1)  # Use BART's input embedding\n",
    "generated_embedding = model.get_input_embeddings()(generated_tensor).mean(dim=1)\n",
    "cosine_score = cosine_similarity([user_input_embedding], [generated_embedding])[0][0]\n",
    "\n",
    "print(\"Cosine Similarity Score:\", cosine_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `6.Top-N Recommendations:`\n",
    "- Select the top-N influencer recommendations with the highest relevance scores.\n",
    "- These top-N recommendations are then presented to the user.\n",
    "\n",
    "\n",
    "<details>\n",
    "<summary><b>Expand: Top-N Recommendations</b></summary>\n",
    "\n",
    "### Selecting Top-N Recommendations\n",
    "- Based on calculated relevance scores, select the top-N recommendations.\n",
    "- N represents the desired number of influencer recommendations to present to the user.\n",
    "\n",
    "### Presentation to Users\n",
    "- Present the selected top-N influencer recommendations to the user.\n",
    "- Display influencer names, attributes, summaries, or other relevant information.\n",
    "\n",
    "### Python Code for Selecting Top-N Recommendations\n",
    "```python\n",
    "import heapq\n",
    "\n",
    "# Example relevance scores for generated recommendations\n",
    "relevance_scores = [0.85, 0.75, 0.92, 0.68, 0.95]\n",
    "\n",
    "# Select top-N recommendations\n",
    "n = 3  # Number of top recommendations\n",
    "top_n_indices = heapq.nlargest(n, range(len(relevance_scores)), key=relevance_scores.__getitem__)\n",
    "\n",
    "# Get top-N recommendations based on indices\n",
    "top_n_recommendations = [generated_recommendations[index] for index in top_n_indices]\n",
    "\n",
    "print(\"Top-N Recommendations:\", top_n_recommendations)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## `7.Personalization (Optional):`\n",
    "- Similar to BERT, you can incorporate personalization by fine-tuning BART with user-specific data.\n",
    "- Include user preferences, interactions, and questionnaire responses in the input.\n",
    "\n",
    "\n",
    "<details>\n",
    "<summary><b>Expand: Personalization (Optional)</b></summary>\n",
    "\n",
    "### Incorporating User-Specific Data\n",
    "- Extend BART's personalization capabilities by fine-tuning with user-specific data.\n",
    "- Include user preferences, interactions, and questionnaire responses in the input.\n",
    "\n",
    "### Enhanced User Embeddings\n",
    "- Combine standard user embeddings with personalized features.\n",
    "- Enhance user embeddings with personalized context, leading to more relevant recommendations.\n",
    "\n",
    "### Fine-Tuning for Personalization\n",
    "- Fine-tune the BART model with user-specific data to capture individual preferences.\n",
    "- The personalized model generates influencer recommendations tailored to each user.\n",
    "\n",
    "### Python Code for Personalization (Fine-Tuning)\n",
    "```python\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer, AdamW\n",
    "import torch\n",
    "\n",
    "# Initialize pre-trained BART model and tokenizer\n",
    "model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large\")\n",
    "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large\")\n",
    "\n",
    "# Example user-specific data\n",
    "user_preferences = \"User A is interested in fashion and beauty content.\"\n",
    "user_responses = \"User A prefers eco-friendly brands and travel content.\"\n",
    "\n",
    "# Tokenize user-specific data\n",
    "user_input_tokens = tokenizer.encode(\"[BOS] \" + user_preferences + \" \" + user_responses + \" [EOS]\",\n",
    "                                    add_special_tokens=False)\n",
    "user_input_tensor = torch.tensor([user_input_tokens])\n",
    "\n",
    "# Fine-tune the model with user-specific data\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    logits = model(user_input_tensor).logits\n",
    "    loss = loss_fn(logits.view(-1, logits.shape[-1]), target_tensor.view(-1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `8.Evaluation and Iteration:`\n",
    "- Evaluate the generated recommendations using appropriate metrics like NDCG, hit-ratio, and user feedback.\n",
    "- Fine-tune the model, experiment with configurations, and iterate for continuous improvement.\n",
    "\n",
    "<details>\n",
    "<summary><b>Expand: Evaluation and Iteration</b></summary>\n",
    "\n",
    "### Performance Evaluation\n",
    "- Evaluate the generated recommendations using appropriate metrics.\n",
    "- Common metrics include Normalized Discounted Cumulative Gain (NDCG), hit-ratio, and user feedback.\n",
    "\n",
    "### NDCG (Normalized Discounted Cumulative Gain)\n",
    "- NDCG measures the ranking quality of recommendations.\n",
    "- Higher NDCG values indicate better-quality recommendations.\n",
    "\n",
    "### Hit-Ratio\n",
    "- Hit-ratio measures the proportion of recommendations that are relevant.\n",
    "- A higher hit-ratio suggests a higher proportion of successful recommendations.\n",
    "\n",
    "### User Feedback\n",
    "- Gather user feedback to understand the quality and relevance of recommendations.\n",
    "- User input helps identify areas for improvement and potential shortcomings.\n",
    "\n",
    "### Fine-Tuning and Iteration\n",
    "- Fine-tune the model based on evaluation results and user feedback.\n",
    "- Experiment with model configurations, hyperparameters, and data preprocessing techniques.\n",
    "\n",
    "### Python Code for NDCG Calculation\n",
    "```python\n",
    "from sklearn.metrics import ndcg_score\n",
    "\n",
    "# Example true relevance labels and predicted relevance scores\n",
    "true_labels = [2, 1, 3]  # Ground truth relevance labels\n",
    "predicted_scores = [0.5, 0.7, 0.9]  # Predicted relevance scores\n",
    "\n",
    "# Calculate NDCG (Normalized Discounted Cumulative Gain)\n",
    "ndcg = ndcg_score([true_labels], [predicted_scores])\n",
    "\n",
    "print(\"NDCG Score:\", ndcg)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
